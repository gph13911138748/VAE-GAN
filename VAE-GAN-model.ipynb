{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfd7dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85aa5a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',None)\n",
    "df = pd.read_csv('final_cohort_variable1_6_shix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67391145",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns[df.isnull().sum()>len(df)/2]:\n",
    "    df.drop([column],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "61a27ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         86.0\n",
      "1         86.0\n",
      "2         78.0\n",
      "3         78.0\n",
      "4         78.0\n",
      "          ... \n",
      "318895    86.0\n",
      "318896    86.0\n",
      "318897    86.0\n",
      "318898    86.0\n",
      "318899    90.0\n",
      "Name: weight, Length: 183464, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bbef45a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>weight</th>\n",
       "      <th>readmission</th>\n",
       "      <th>hospital_expire_flag</th>\n",
       "      <th>sirs</th>\n",
       "      <th>charlson</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>sbp</th>\n",
       "      <th>mbp</th>\n",
       "      <th>dbp</th>\n",
       "      <th>temperature</th>\n",
       "      <th>spo2</th>\n",
       "      <th>gcs</th>\n",
       "      <th>is_ventilation</th>\n",
       "      <th>urine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.834640e+05</td>\n",
       "      <td>1.834640e+05</td>\n",
       "      <td>1.834640e+05</td>\n",
       "      <td>183464.000000</td>\n",
       "      <td>183464.000000</td>\n",
       "      <td>183464.000000</td>\n",
       "      <td>183464.000000</td>\n",
       "      <td>183464.000000</td>\n",
       "      <td>183464.000000</td>\n",
       "      <td>183464.000000</td>\n",
       "      <td>183464.000000</td>\n",
       "      <td>183464.000000</td>\n",
       "      <td>183464.000000</td>\n",
       "      <td>183464.000000</td>\n",
       "      <td>183464.000000</td>\n",
       "      <td>183464.000000</td>\n",
       "      <td>183464.000000</td>\n",
       "      <td>183464.000000</td>\n",
       "      <td>183464.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.498804e+07</td>\n",
       "      <td>2.498282e+07</td>\n",
       "      <td>3.447505e+07</td>\n",
       "      <td>65.236406</td>\n",
       "      <td>0.559903</td>\n",
       "      <td>81.627287</td>\n",
       "      <td>0.242816</td>\n",
       "      <td>0.106190</td>\n",
       "      <td>2.727363</td>\n",
       "      <td>5.893336</td>\n",
       "      <td>84.553868</td>\n",
       "      <td>118.869591</td>\n",
       "      <td>78.249720</td>\n",
       "      <td>62.846143</td>\n",
       "      <td>36.917822</td>\n",
       "      <td>96.855566</td>\n",
       "      <td>14.515098</td>\n",
       "      <td>0.075688</td>\n",
       "      <td>350.189133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.886890e+06</td>\n",
       "      <td>2.885762e+06</td>\n",
       "      <td>2.892803e+06</td>\n",
       "      <td>17.027097</td>\n",
       "      <td>0.496400</td>\n",
       "      <td>45.426953</td>\n",
       "      <td>0.428786</td>\n",
       "      <td>0.308081</td>\n",
       "      <td>0.936270</td>\n",
       "      <td>3.232204</td>\n",
       "      <td>16.934931</td>\n",
       "      <td>18.038499</td>\n",
       "      <td>12.540471</td>\n",
       "      <td>12.432201</td>\n",
       "      <td>0.600068</td>\n",
       "      <td>2.445178</td>\n",
       "      <td>1.389474</td>\n",
       "      <td>0.264499</td>\n",
       "      <td>304.617894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000122e+07</td>\n",
       "      <td>2.000009e+07</td>\n",
       "      <td>3.000015e+07</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.750000</td>\n",
       "      <td>39.500000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.750000</td>\n",
       "      <td>29.900000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5850.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.248812e+07</td>\n",
       "      <td>2.248675e+07</td>\n",
       "      <td>3.192143e+07</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>65.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>72.250000</td>\n",
       "      <td>105.750000</td>\n",
       "      <td>69.400000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>95.500000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.498919e+07</td>\n",
       "      <td>2.496327e+07</td>\n",
       "      <td>3.420324e+07</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>78.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>116.750000</td>\n",
       "      <td>76.750000</td>\n",
       "      <td>61.500000</td>\n",
       "      <td>36.890000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>260.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.749902e+07</td>\n",
       "      <td>2.747596e+07</td>\n",
       "      <td>3.690966e+07</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>95.250000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>85.670000</td>\n",
       "      <td>70.250000</td>\n",
       "      <td>37.220000</td>\n",
       "      <td>98.750000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>450.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.999999e+07</td>\n",
       "      <td>2.999983e+07</td>\n",
       "      <td>3.999981e+07</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6492.790000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>177.110000</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>202.250000</td>\n",
       "      <td>161.670000</td>\n",
       "      <td>41.330000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5550.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         subject_id       hadm_id       stay_id            age         gender  \\\n",
       "count  1.834640e+05  1.834640e+05  1.834640e+05  183464.000000  183464.000000   \n",
       "mean   1.498804e+07  2.498282e+07  3.447505e+07      65.236406       0.559903   \n",
       "std    2.886890e+06  2.885762e+06  2.892803e+06      17.027097       0.496400   \n",
       "min    1.000122e+07  2.000009e+07  3.000015e+07      18.000000       0.000000   \n",
       "25%    1.248812e+07  2.248675e+07  3.192143e+07      55.000000       0.000000   \n",
       "50%    1.498919e+07  2.496327e+07  3.420324e+07      67.000000       1.000000   \n",
       "75%    1.749902e+07  2.747596e+07  3.690966e+07      78.000000       1.000000   \n",
       "max    1.999999e+07  2.999983e+07  3.999981e+07      99.000000       1.000000   \n",
       "\n",
       "              weight    readmission  hospital_expire_flag           sirs  \\\n",
       "count  183464.000000  183464.000000         183464.000000  183464.000000   \n",
       "mean       81.627287       0.242816              0.106190       2.727363   \n",
       "std        45.426953       0.428786              0.308081       0.936270   \n",
       "min         1.000000       0.000000              0.000000       0.000000   \n",
       "25%        65.900000       0.000000              0.000000       2.000000   \n",
       "50%        78.200000       0.000000              0.000000       3.000000   \n",
       "75%        93.000000       0.000000              0.000000       3.000000   \n",
       "max      6492.790000       1.000000              1.000000       4.000000   \n",
       "\n",
       "            charlson     heart_rate            sbp            mbp  \\\n",
       "count  183464.000000  183464.000000  183464.000000  183464.000000   \n",
       "mean        5.893336      84.553868     118.869591      78.249720   \n",
       "std         3.232204      16.934931      18.038499      12.540471   \n",
       "min         0.000000      27.750000      39.500000      14.000000   \n",
       "25%         4.000000      72.250000     105.750000      69.400000   \n",
       "50%         6.000000      83.000000     116.750000      76.750000   \n",
       "75%         8.000000      95.250000     130.000000      85.670000   \n",
       "max        20.000000     177.110000     228.000000     202.250000   \n",
       "\n",
       "                 dbp    temperature           spo2            gcs  \\\n",
       "count  183464.000000  183464.000000  183464.000000  183464.000000   \n",
       "mean       62.846143      36.917822      96.855566      14.515098   \n",
       "std        12.432201       0.600068       2.445178       1.389474   \n",
       "min        14.750000      29.900000       9.000000       3.000000   \n",
       "25%        54.000000      36.600000      95.500000      15.000000   \n",
       "50%        61.500000      36.890000      97.000000      15.000000   \n",
       "75%        70.250000      37.220000      98.750000      15.000000   \n",
       "max       161.670000      41.330000     100.000000      15.000000   \n",
       "\n",
       "       is_ventilation          urine  \n",
       "count   183464.000000  183464.000000  \n",
       "mean         0.075688     350.189133  \n",
       "std          0.264499     304.617894  \n",
       "min          0.000000   -5850.000000  \n",
       "25%          0.000000     150.000000  \n",
       "50%          0.000000     260.000000  \n",
       "75%          0.000000     450.000000  \n",
       "max          1.000000    5550.000000  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dea88c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "miss = df.isnull().sum(axis=1)\n",
    "df.drop(index=miss[miss>0].index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc619c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "mis = df['subject_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "017498e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "subject_id = df['subject_id'].value_counts().index\n",
    "num = np.array(df['subject_id'].value_counts())\n",
    "count = pd.DataFrame(np.vstack([subject_id,num]).T,columns=['subject_id','count'])\n",
    "df2 = pd.merge(df,count,how='inner',on='subject_id')\n",
    "mis = df2['count']<6\n",
    "df3 = df2.drop(index=mis[mis].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbb77c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.index = range(len(df3))\n",
    "X = df3.copy()\n",
    "X = X.drop_duplicates(['subject_id'])\n",
    "X = X.iloc[:,[4,5,6,7,8,9,10]]\n",
    "df_died = X['hospital_expire_flag']#用来做决策任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3f064c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn import metrics\n",
    "# kmeans_model = KMeans(n_clusters=5, random_state=10).fit(X)\n",
    "# labels = kmeans_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20212394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bba51156",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df3['label']=df_died\n",
    "# for i in X.index:\n",
    "#     for j in range(6):\n",
    "#         df3.loc[i+j,'label']=X.loc[i,'label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99c7b41",
   "metadata": {},
   "source": [
    "确认用什么标准化很关键"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee596ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df3.iloc[:,range(11,20)]\n",
    "data = (data-data.min())/(data.max()-data.min())\n",
    "#data = (data - data.mean())/data.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afcdd7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label = df3.iloc[:,21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f9e5d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "Label = df_died"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6197dafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "data_iter = []\n",
    "for i in range(0,len(data),6):\n",
    "    data_iter.append(np.array(data.iloc[i:i+6,:]).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72082bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9926, 1, 6, 9])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_iter = torch.Tensor(data_iter)\n",
    "data_iter = data_iter.unsqueeze(1)\n",
    "data_iter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c9d8990",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter_train = data_iter[:7000]\n",
    "data_iter_test  = data_iter[9626:9926]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95783e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9926])\n"
     ]
    }
   ],
   "source": [
    "Label = torch.Tensor(Label)\n",
    "print(Label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97f3f8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Label1 = Label[:7000]\n",
    "Label2 = Label[9626:9926]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6ad0d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "torch_dataset = Data.TensorDataset(data_iter_train,Label1)\n",
    "loader = Data.DataLoader(dataset = torch_dataset,batch_size=128,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0e58aa",
   "metadata": {},
   "source": [
    "VAE－LSTM－GAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377c693d",
   "metadata": {},
   "source": [
    "1.matrix initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1c30bb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "missing_rate = 0.6#设置随机缺失率\n",
    "\n",
    "def matrix_init(mat):\n",
    "    \"\"\"\n",
    "    获取mask矩阵binary_matrix，随机缺失seed矩阵random_matrix,和随机缺失矩阵sparse_matrix\n",
    "    \"\"\"\n",
    "    high = mat.shape[0]\n",
    "    row = mat.shape[1]\n",
    "    col = mat.shape[2]\n",
    "    \n",
    "    binary_matrix = np.zeros((high, row, col))\n",
    "    random_matrix = np.random.random((high, row, col))\n",
    "    for h in range(high):\n",
    "        for i in range(row):\n",
    "            for j in range(col):\n",
    "                binary_matrix[h, i, j] = np.round(random_matrix[h, i,j] + 0.5 - missing_rate)\n",
    "    \n",
    "    sparse_matrix = np.multiply(mat, binary_matrix)\n",
    "    return torch.tensor(binary_matrix), sparse_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce4f3f8",
   "metadata": {},
   "source": [
    "2.Neural Network decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943d8b19",
   "metadata": {},
   "source": [
    "initial filling based on time regular data(up to down)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb61afa3",
   "metadata": {},
   "source": [
    "SVD decomposition                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f1c0bc",
   "metadata": {},
   "source": [
    "filling NaN in sparse_matrix(multiple with binary_matrix and don't change the orginal data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3934a8b5",
   "metadata": {},
   "source": [
    "get the first-repaired matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47131ec",
   "metadata": {},
   "source": [
    "this is the input in VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03957df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def means_filling(mat):\n",
    "    \"\"\"\n",
    "    先用均值进行填充\n",
    "    pay attention to the data type in np.array\n",
    "    \"\"\"\n",
    "    for h in range(mat.shape[0]):\n",
    "        means = mat.sum(axis=1)/((mat>0).sum(axis=1)+1)\n",
    "        for i in range(mat.shape[1]):\n",
    "            for j in range(mat.shape[2]):\n",
    "                if mat[h, i, j] == 0:\n",
    "                    mat[h, i, j] = means[h, j]\n",
    "    return torch.tensor(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9449b356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "def SVD_func(full_matrix, binary_matrix, sparse_matrix):\n",
    "    \"\"\"\n",
    "    返还初次填补矩阵\n",
    "    \"\"\"\n",
    "    mat = np.zeros((full_matrix.shape[0], full_matrix.shape[1], full_matrix.shape[2]))\n",
    "    for h in range(full_matrix.shape[0]):\n",
    "        u, s, k = linalg.svd(full_matrix[h].reshape((full_matrix.shape[1],full_matrix.shape[2])))\n",
    "        mat[h,:s.shape[0], :s.shape[0]] = np.diag(s)\n",
    "        mat[h] = np.matmul(u, mat[h])\n",
    "        mat[h] = np.matmul(mat[h], k)\n",
    "    binary_matrix_reverse = torch.tensor(np.ones((binary_matrix.shape[0], binary_matrix.shape[1], binary_matrix.shape[2])))-binary_matrix\n",
    "    return np.multiply(mat, binary_matrix_reverse) + sparse_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c08a783",
   "metadata": {},
   "source": [
    "3.VAE-GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514d7570",
   "metadata": {},
   "source": [
    "VAE structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbe363c",
   "metadata": {},
   "source": [
    "Time Convolution - Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7815cc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, sampler\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms as tfs\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "#VAE结构如下，也是生成器\n",
    "\n",
    "class Reshape(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(-1, 6, 9)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.f1 = Reshape()\n",
    "        self.fc1 = nn.Conv1d(1, 1, kernel_size = (4, 1))\n",
    "        self.fc2 = nn.Conv1d(1, 1, kernel_size = (3, 1))\n",
    "        self.fc21 = nn.Linear(9, 9) # mean\n",
    "        self.fc22 = nn.Linear(9, 9) # var\n",
    "        self.fc3 = nn.ConvTranspose1d(1, 1, kernel_size = (3, 1))\n",
    "        self.fc4 = nn.ConvTranspose1d(1, 1, kernel_size = (4, 1))\n",
    "\n",
    "    def encode(self, x): #编码层\n",
    "        h1 = torch.relu(self.fc1(x))\n",
    "        h1 = torch.relu(self.fc2(h1))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_() #e**(x/2)\n",
    "        eps = torch.FloatTensor(std.size()).normal_()\n",
    "        if torch.cuda.is_available():\n",
    "            eps = Variable(eps.cuda())\n",
    "        else:\n",
    "            eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):#解码层\n",
    "        h3 = torch.relu(self.fc3(z))\n",
    "        return torch.tanh(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x) # 编码\n",
    "        z = self.reparametrize(mu, logvar) # 重新参数化成正态分布\n",
    "        return self.decode(z), mu, logvar # 解码，同时输出均值方差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffcc41b",
   "metadata": {},
   "source": [
    "LSTM - Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdc9fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator,self).__init__()\n",
    "            #Input:9926*1*6*9\n",
    "#         self.lstm = nn.LSTM(#类比每次输入1个字，6个时间点，每个字是9个维度，输出为隐藏层数目27\n",
    "#                 9,\n",
    "#                 27,\n",
    "#                 2, #层数为2\n",
    "#                 batch_first=True\n",
    "#             )#4*4\n",
    "#         self.l1 = nn.Linear(162, 128)\n",
    "#         self.l3 = nn.Linear(128, 54)\n",
    "#         self.l2 = Reshape()\n",
    "#         self.l4 = nn.Dropout(p=0.2)\n",
    "# #         可以考虑用softmax\n",
    "#         for name, param in self.lstm.named_parameters():\n",
    "#             nn.init.uniform_(param,0,0.1)\n",
    "\n",
    "        self.l1 = nn.Conv1d(1, 1, kernel_size = (4, 1))\n",
    "        self.l2 = nn.Conv1d(1, 1, kernel_size = (3, 1))\n",
    "        self.l3 = nn.Linear(9 , 27)\n",
    "        self.l4 = nn.Linear(27, 54)\n",
    "        self.l5 = Reshape()\n",
    "    \n",
    "    def forward(self,x):\n",
    "#         out, _ = self.lstm(x)\n",
    "        #out = out[:, -1, :] #可以考虑对参数进行分类\n",
    "#         out = torch.sigmoid(out)\n",
    "#         out = nn.Flatten(out)\n",
    "#         out = out.reshape(-1, 162)\n",
    "#         ln1 = self.l1(out)\n",
    "#         ln2 = self.l3(ln1)\n",
    "#         ln2 = torch.sigmoid(ln2)\n",
    "#         ln2 = self.l2(ln2)\n",
    "#         return ln2\n",
    "        x = x.view(-1,1,6,9)\n",
    "        out = self.l1(x)\n",
    "        out = self.l2(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.l4(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return self.l5(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce711719",
   "metadata": {},
   "source": [
    "Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040ee65a",
   "metadata": {},
   "source": [
    "$$\\hat{X}=M\\circledast\\tilde{X}+(1-M)\\circledast G\\left(\\tilde{X},M\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7bde011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls_discriminator_loss(discirminator_matrix, binary_matirx): #判别器的loss\n",
    "#     print(binary_matirx.shape)\n",
    "#     binary_matirx = binary_matirx.reshape(1,binary_matirx.shape[1],binary_matirx.shape[2])\n",
    "#     binary_matirx = binary_matirx.expand(1000,binary_matirx.shape[1],binary_matirx.shape[2])\n",
    "    loss = -sum(sum(sum(binary_matirx * torch.log(discirminator_matrix + 1e-8) + (1 - binary_matirx) * torch.log(1. - discirminator_matrix + 1e-8))))\n",
    "    return loss/(binary_matirx.shape[0] * binary_matirx.shape[1] * binary_matirx.shape[2])\n",
    "\n",
    "def MSE_train_loss(binary_matrix, origin_matrix, G_sample):\n",
    "    loss = sum(sum(sum(( binary_matrix * origin_matrix - binary_matrix * G_sample) ** 2)))/sum(sum(sum(binary_matrix)))\n",
    "    return loss\n",
    "\n",
    "def ls_generator_loss(discirminator_matrix, binary_matirx):\n",
    "#     binary_matirx = binary_matirx.reshape(1,binary_matirx.shape[1],binary_matirx.shape[2])\n",
    "#     binary_matirx = binary_matirx.expand(1000,binary_matirx.shape[1],binary_matirx.shape[2])\n",
    "    loss = -sum(sum(sum((1 - binary_matirx) * torch.log(discirminator_matrix + 1e-8))))\n",
    "    return loss/(binary_matirx.shape[0] * binary_matirx.shape[1] * binary_matirx.shape[2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c74568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#先reshape将二维张量转化为三维张量，然后再将三维张量平移扩展expand达到赋值二维张量到三维的操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b09e32a",
   "metadata": {},
   "source": [
    "Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44ae3b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(net):\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=3e-3, betas=(0.5, 0.999))\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31419a4d",
   "metadata": {},
   "source": [
    "MSE_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5285d598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_train(binary_matrix, origin_matrix, G_sample):\n",
    "    loss = sum(sum(sum(((1 - binary_matrix) * origin_matrix - (1 - binary_matrix) * G_sample) ** 2)))/sum(sum(sum(1 - binary_matrix)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a5bead",
   "metadata": {},
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3fbd4f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "def train_a_gan(D_net, G_net, D_optimizer, G_optimizer, discriminator_loss, generator_loss, show_every=9926, num_epochs=100):\n",
    "    iter_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for x, (real_data,_) in enumerate(loader):\n",
    "            #real_data = x.reshape(-1,6,9)\n",
    "            #bs = x.shape[0]   #128\n",
    "            # 判别网络\n",
    "            #real_data = Variable(x).view(bs, -1) \n",
    "            binary_matrix, sparse_matrix = matrix_init(real_data.reshape(-1,6,9))\n",
    "            fulling_matrix = SVD_func(means_filling(sparse_matrix),binary_matrix,sparse_matrix)\n",
    "            fulling_matrix = fulling_matrix.reshape(-1,1,6,9).to(torch.float32)\n",
    "            logits_real = D_net(real_data.reshape(-1,6,9)) # 判别网络得分\n",
    "            fake_images, mu, logvar = G_net(fulling_matrix) # 生成的假的数据\n",
    "            logits_fake = D_net(fake_images.reshape(-1,6,9)) # 判别网络得分\n",
    "\n",
    "            d_total_error = discriminator_loss(logits_fake,binary_matrix) \n",
    "            d_total_error += discriminator_loss(logits_real,torch.tensor(np.ones((binary_matrix.shape[0], binary_matrix.shape[1], binary_matrix.shape[2])))) \n",
    "            D_optimizer.zero_grad()\n",
    "            d_total_error.backward()\n",
    "            D_optimizer.step() # 优化判别网络\n",
    "            \n",
    "            # 生成网络\n",
    "            fake_images,mu, logvar = G_net(fulling_matrix) # 生成的假的数据\n",
    "            gen_logits_fake = D_net(fake_images.reshape(-1,6,9))\n",
    "            g_error = generator_loss(gen_logits_fake,binary_matrix)+ 5 * MSE_train_loss(binary_matrix, real_data.reshape(-1,6,9) , fake_images.reshape(-1,6,9)) # 生成网络的 loss\n",
    "            G_optimizer.zero_grad()\n",
    "            g_error.backward()\n",
    "            G_optimizer.step() # 优化生成网络\n",
    "\n",
    "            ##if (iter_count % show_every == 0):\n",
    "        #if epoch%9==0:\n",
    "        print('Iter: {}, D: {:.4}, G:{:.4}'.format(iter_count, d_total_error.item(), g_error.item()))\n",
    "#                 imgs_numpy = deprocess_img(fake_images.data.cpu().numpy())\n",
    "#                 show_images(imgs_numpy[0:16])\n",
    "#                 plt.show()\n",
    "#       print(MSE_train(binary_matrix,real_data.reshape(-1,6,9),fake_images))\n",
    "        print(epoch)\n",
    "        iter_count += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7340b87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-bd69fe4fb5b3>:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(mat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, D: 0.5736, G:1.423\n",
      "0\n",
      "Iter: 1, D: 0.5353, G:0.8651\n",
      "1\n",
      "Iter: 2, D: 0.5356, G:0.5915\n",
      "2\n",
      "Iter: 3, D: 0.4992, G:0.5333\n",
      "3\n",
      "Iter: 4, D: 0.5365, G:0.4512\n",
      "4\n",
      "Iter: 5, D: 0.5679, G:0.3236\n",
      "5\n",
      "Iter: 6, D: 0.5736, G:0.2301\n",
      "6\n",
      "Iter: 7, D: 0.534, G:0.1838\n",
      "7\n",
      "Iter: 8, D: 0.5685, G:0.1794\n",
      "8\n",
      "Iter: 9, D: 0.5373, G:0.1827\n",
      "9\n",
      "Iter: 10, D: 0.551, G:0.1624\n",
      "10\n",
      "Iter: 11, D: 0.5471, G:0.1906\n",
      "11\n",
      "Iter: 12, D: 0.5363, G:0.1783\n",
      "12\n",
      "Iter: 13, D: 0.5346, G:0.1682\n",
      "13\n",
      "Iter: 14, D: 0.5634, G:0.1755\n",
      "14\n",
      "Iter: 15, D: 0.5903, G:0.1676\n",
      "15\n",
      "Iter: 16, D: 0.5356, G:0.1568\n",
      "16\n",
      "Iter: 17, D: 0.6011, G:0.1492\n",
      "17\n",
      "Iter: 18, D: 0.5956, G:0.146\n",
      "18\n",
      "Iter: 19, D: 0.6045, G:0.1351\n",
      "19\n",
      "Iter: 20, D: 0.6244, G:0.1277\n",
      "20\n",
      "Iter: 21, D: 0.6413, G:0.1202\n",
      "21\n",
      "Iter: 22, D: 0.6327, G:0.1085\n",
      "22\n",
      "Iter: 23, D: 0.6613, G:0.1237\n",
      "23\n",
      "Iter: 24, D: 0.6422, G:0.1164\n",
      "24\n",
      "Iter: 25, D: 0.6325, G:0.104\n",
      "25\n",
      "Iter: 26, D: 0.6352, G:0.1077\n",
      "26\n",
      "Iter: 27, D: 0.6346, G:0.105\n",
      "27\n",
      "Iter: 28, D: 0.636, G:0.0903\n",
      "28\n",
      "Iter: 29, D: 0.6482, G:0.1107\n",
      "29\n",
      "Iter: 30, D: 0.6543, G:0.1174\n",
      "30\n",
      "Iter: 31, D: 0.6483, G:0.09679\n",
      "31\n",
      "Iter: 32, D: 0.6365, G:0.1106\n",
      "32\n",
      "Iter: 33, D: 0.6316, G:0.09619\n",
      "33\n",
      "Iter: 34, D: 0.6266, G:0.1049\n",
      "34\n",
      "Iter: 35, D: 0.6526, G:0.1035\n",
      "35\n",
      "Iter: 36, D: 0.6468, G:0.1024\n",
      "36\n",
      "Iter: 37, D: 0.6492, G:0.09645\n",
      "37\n",
      "Iter: 38, D: 0.6498, G:0.09863\n",
      "38\n",
      "Iter: 39, D: 0.6485, G:0.1024\n",
      "39\n",
      "Iter: 40, D: 0.661, G:0.1091\n",
      "40\n",
      "Iter: 41, D: 0.6636, G:0.08123\n",
      "41\n",
      "Iter: 42, D: 0.6622, G:0.1111\n",
      "42\n",
      "Iter: 43, D: 0.67, G:0.09472\n",
      "43\n",
      "Iter: 44, D: 0.6281, G:0.1009\n",
      "44\n",
      "Iter: 45, D: 0.6633, G:0.1027\n",
      "45\n",
      "Iter: 46, D: 0.6632, G:0.09392\n",
      "46\n",
      "Iter: 47, D: 0.6432, G:0.09206\n",
      "47\n",
      "Iter: 48, D: 0.6579, G:0.09863\n",
      "48\n",
      "Iter: 49, D: 0.6397, G:0.1029\n",
      "49\n",
      "Iter: 50, D: 0.6397, G:0.09539\n",
      "50\n",
      "Iter: 51, D: 0.6436, G:0.08777\n",
      "51\n",
      "Iter: 52, D: 0.65, G:0.09426\n",
      "52\n",
      "Iter: 53, D: 0.6393, G:0.1061\n",
      "53\n",
      "Iter: 54, D: 0.6697, G:0.09879\n",
      "54\n",
      "Iter: 55, D: 0.6639, G:0.1064\n",
      "55\n",
      "Iter: 56, D: 0.635, G:0.08879\n",
      "56\n",
      "Iter: 57, D: 0.6463, G:0.09247\n",
      "57\n",
      "Iter: 58, D: 0.6558, G:0.09225\n",
      "58\n",
      "Iter: 59, D: 0.6508, G:0.08854\n",
      "59\n",
      "Iter: 60, D: 0.666, G:0.1103\n",
      "60\n",
      "Iter: 61, D: 0.6619, G:0.07903\n",
      "61\n",
      "Iter: 62, D: 0.6723, G:0.09364\n",
      "62\n",
      "Iter: 63, D: 0.6574, G:0.0849\n",
      "63\n",
      "Iter: 64, D: 0.6506, G:0.09695\n",
      "64\n",
      "Iter: 65, D: 0.6347, G:0.1065\n",
      "65\n",
      "Iter: 66, D: 0.6585, G:0.09034\n",
      "66\n",
      "Iter: 67, D: 0.6579, G:0.1006\n",
      "67\n",
      "Iter: 68, D: 0.6508, G:0.08399\n",
      "68\n",
      "Iter: 69, D: 0.6652, G:0.1175\n",
      "69\n",
      "Iter: 70, D: 0.634, G:0.1013\n",
      "70\n",
      "Iter: 71, D: 0.6576, G:0.0965\n",
      "71\n",
      "Iter: 72, D: 0.6634, G:0.1104\n",
      "72\n",
      "Iter: 73, D: 0.6551, G:0.0971\n",
      "73\n",
      "Iter: 74, D: 0.6701, G:0.09036\n",
      "74\n",
      "Iter: 75, D: 0.6388, G:0.09895\n",
      "75\n",
      "Iter: 76, D: 0.6687, G:0.09465\n",
      "76\n",
      "Iter: 77, D: 0.667, G:0.09539\n",
      "77\n",
      "Iter: 78, D: 0.6411, G:0.09896\n",
      "78\n",
      "Iter: 79, D: 0.6459, G:0.09265\n",
      "79\n",
      "Iter: 80, D: 0.6622, G:0.0974\n",
      "80\n",
      "Iter: 81, D: 0.6216, G:0.08556\n",
      "81\n",
      "Iter: 82, D: 0.6548, G:0.09264\n",
      "82\n",
      "Iter: 83, D: 0.652, G:0.0943\n",
      "83\n",
      "Iter: 84, D: 0.678, G:0.1157\n",
      "84\n",
      "Iter: 85, D: 0.6589, G:0.1004\n",
      "85\n",
      "Iter: 86, D: 0.6521, G:0.09552\n",
      "86\n",
      "Iter: 87, D: 0.6609, G:0.08991\n",
      "87\n",
      "Iter: 88, D: 0.6381, G:0.08749\n",
      "88\n",
      "Iter: 89, D: 0.6492, G:0.09655\n",
      "89\n",
      "Iter: 90, D: 0.6701, G:0.09385\n",
      "90\n",
      "Iter: 91, D: 0.6588, G:0.102\n",
      "91\n",
      "Iter: 92, D: 0.6442, G:0.1023\n",
      "92\n",
      "Iter: 93, D: 0.6313, G:0.0934\n",
      "93\n",
      "Iter: 94, D: 0.6342, G:0.09668\n",
      "94\n",
      "Iter: 95, D: 0.6444, G:0.1058\n",
      "95\n",
      "Iter: 96, D: 0.63, G:0.09046\n",
      "96\n",
      "Iter: 97, D: 0.6468, G:0.1031\n",
      "97\n",
      "Iter: 98, D: 0.6648, G:0.1006\n",
      "98\n",
      "Iter: 99, D: 0.649, G:0.09548\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "D = Discriminator()\n",
    "G = VAE()\n",
    "\n",
    "D_optim = get_optimizer(D)\n",
    "G_optim = get_optimizer(G)\n",
    "\n",
    "train_a_gan(D, G, D_optim, G_optim, ls_discriminator_loss, ls_generator_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d550eb6",
   "metadata": {},
   "source": [
    "calculate MSE with methods of SVD-VAE-GAN and  other methods "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7513f9",
   "metadata": {},
   "source": [
    "SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9996556",
   "metadata": {},
   "source": [
    "MICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8fb79b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import miceforest as mf\n",
    "def miceforest_test(sparse_matrix):\n",
    "    sparse_matrix = sparse_matrix.to(torch.float32)\n",
    "    sparse_matrix[sparse_matrix==0]=np.nan\n",
    "    #sparse_matrix[:,:,7]=0\n",
    "    sparse_matrix = sparse_matrix.reshape(-1,9)\n",
    "    \n",
    "    j = np.array(sparse_matrix)\n",
    "    j = pd.DataFrame(j,columns=[str(k) for k in range(sparse_matrix.shape[1])])\n",
    "\n",
    "    kernel = mf.MultipleImputedKernel(\n",
    "        j,\n",
    "        datasets=9,\n",
    "        save_all_iterations=True,\n",
    "        random_state=1991\n",
    "    )\n",
    "    kernel.mice(8)\n",
    "    kernell = kernel.complete_data(8)\n",
    "\n",
    "    ii = np.array(kernell)\n",
    "    sparse_matrix = torch.tensor(ii)\n",
    "    return sparse_matrix.reshape(-1,6,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdd1ffa",
   "metadata": {},
   "source": [
    "MissForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b67b792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MissForestExtra:\n",
    "    def __init__(self):     \n",
    "        pass\n",
    "    \n",
    "    def impute(self, df, impute, model, max_iter=5, verbose=0):\n",
    "        df3 = df.copy()\n",
    "        pd.options.mode.chained_assignment = None  # default='warn'\n",
    "                  \n",
    "        encode_col = [f for f in df.columns if isinstance(df[f].sample(1).values[0], str)]\n",
    "        for c in encode_col:  \n",
    "            df3[c].replace(df[c].dropna().unique(), range(df[c].dropna().nunique()), inplace=True) # label encoding\n",
    "            if c == impute:\n",
    "                impute_map = ({k:v for (k,v) in zip(df[c].dropna().unique(), range(df[c].dropna().nunique()))})\n",
    "                impute_map = {v:k for (k,v) in impute_map.items()}\n",
    "       \n",
    "        non_null = list(set([feat for feat, val in zip(df.isnull().sum().index, df.isnull().sum()) if val == 0] + [impute]))\n",
    "            \n",
    "        track = {}\n",
    "        for n_iter in range(max_iter):\n",
    "            df2=df3[non_null].copy()\n",
    "            df2['Training/Predict'] = np.where(df2[impute].isnull(), 'predict', 'training')\n",
    "            predict_set = df2[df2['Training/Predict'] == 'predict']\n",
    "            training_set = df2[df2['Training/Predict'] == 'training']\n",
    "\n",
    "            if n_iter == 1:\n",
    "                predict_set[impute].fillna(df2[impute].median(), inplace=True) if impute in encode_col else predict_set[impute].fillna(df2[impute].mode().values[0], inplace=True)\n",
    "                   \n",
    "            if n_iter > 1:\n",
    "                for idx, pred in zip(predict_set.index, res):\n",
    "                    predict_set.loc[idx, impute] = pred\n",
    "           \n",
    "            model.fit(training_set[non_null].drop(impute, axis=1), training_set[impute])\n",
    "            if predict_set[non_null].drop(impute, axis=1).shape[0] != 0:\n",
    "                res = model.predict(predict_set[non_null].drop(impute, axis=1))\n",
    "            else:\n",
    "                raise Exception(f'Feature \"{impute}\" does not contain any missing values.')\n",
    "                \n",
    "            track[f'iter{n_iter}'] = res\n",
    "            \n",
    "            if n_iter > 1:       \n",
    "                if (track[f'iter{n_iter}'] == track[f'iter{n_iter - 1}']).all():\n",
    "                    if verbose == 1:\n",
    "                        print('Stopping Criteria Reached (no change in imputed values)')\n",
    "                    break\n",
    "            if verbose == 1:       \n",
    "                if max_iter <= 1:\n",
    "                    print('Stopping Criteria Reached (max iter reached)')\n",
    "                    break\n",
    "                   \n",
    "            n_iter += 1\n",
    "           \n",
    "        pd.options.mode.chained_assignment = 'warn'  \n",
    "       \n",
    "        imputed_feature = pd.concat([training_set[impute], predict_set[impute]], axis=0).sort_index()\n",
    "        return imputed_feature.replace(impute_map) if impute in encode_col else imputed_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8daa1373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Missforest_test(sparse_matrix):\n",
    "    from xgboost import XGBRegressor\n",
    "    xgbr = XGBRegressor(use_label_encoder=False, eval_metric='merror')\n",
    "    mfe = MissForestExtra()\n",
    "    sparse_matrix = sparse_matrix.to(torch.float32)\n",
    "    sparse_matrix[sparse_matrix==0]=np.nan\n",
    "    sparse_matrix[:,:,7]=0#有一列是全0的离散序列，所以必须要返还，不然无法做XGB\n",
    "    sparse_matrix = sparse_matrix.reshape(-1,9)\n",
    "    \n",
    "    j = np.array(sparse_matrix)\n",
    "    j = pd.DataFrame(j,columns=[str(k) for k in range(sparse_matrix.shape[1])])\n",
    "\n",
    "    for k in j.columns:\n",
    "        if j[k].isnull().sum()>0:\n",
    "            j[k] = mfe.impute(j, k, xgbr)\n",
    "    ii = np.array(j)\n",
    "    sparse_matrix = torch.tensor(ii)\n",
    "    return sparse_matrix.reshape(-1,6,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9460461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputee(sparse_matrix):\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    sparse_matrix = sparse_matrix.to(torch.float32)\n",
    "    sparse_matrix[sparse_matrix==0]=np.nan\n",
    "    sparse_matrix[:,:,7]=0#有一列是全0的离散序列，所以必须要返还，不然无法做XGB\n",
    "    sparse_matrix = sparse_matrix.reshape(-1,9)\n",
    "    \n",
    "    j = np.array(sparse_matrix)\n",
    "    j = pd.DataFrame(j,columns=[str(k) for k in range(sparse_matrix.shape[1])])\n",
    "\n",
    "    for k in j.columns:\n",
    "        if j.loc[:,k].isnull().sum()>0:\n",
    "            imp_median = SimpleImputer(strategy=\"mean\")\n",
    "            j.loc[:,k] = imp_median.fit_transform(j.loc[:,k].values.reshape(-1,1))\n",
    "    ii = np.array(j)\n",
    "    sparse_matrix = torch.tensor(ii)\n",
    "    return sparse_matrix.reshape(-1,6,9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "793d621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputee_linear(sparse_matrix):\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    sparse_matrix = sparse_matrix.to(torch.float32)\n",
    "    sparse_matrix[sparse_matrix==0]=np.nan\n",
    "    sparse_matrix[:,:,7]=0#有一列是全0的离散序列，所以必须要返还，不然无法做XGB\n",
    "    sparse_matrix = sparse_matrix.reshape(-1,9)\n",
    "    \n",
    "    j = np.array(sparse_matrix)\n",
    "    j = pd.DataFrame(j,columns=[str(k) for k in range(sparse_matrix.shape[1])])\n",
    "\n",
    "    j=j.interpolate(method='polynomial',order=5)\n",
    "    for k in j.columns:\n",
    "        if j.loc[:,k].isnull().sum()>0:\n",
    "            imp_median = SimpleImputer(strategy=\"mean\")\n",
    "            j.loc[:,k] = imp_median.fit_transform(j.loc[:,k].values.reshape(-1,1))\n",
    "    ii = np.array(j)\n",
    "    sparse_matrix = torch.tensor(ii)\n",
    "    return sparse_matrix.reshape(-1,6,9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "744274b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_train2(binary_matrix, origin_matrix, G_sample):\n",
    "    loss = sum(sum(((1 - binary_matrix) * origin_matrix - (1 - binary_matrix) * G_sample) ** 2))/sum(sum(1 - binary_matrix))\n",
    "    return loss\n",
    "\n",
    "def test(data_iter, Label, G_net):\n",
    "    torch_dataset = Data.TensorDataset(data_iter, Label)\n",
    "    loader = Data.DataLoader(dataset = torch_dataset,batch_size=data.shape[0],shuffle=True)\n",
    "    \n",
    "    for x, (real_data,_) in enumerate(loader):\n",
    "        binary_matrix, sparse_matrix = matrix_init(real_data.reshape(-1,6,9))\n",
    "        fulling_matrix = SVD_func(means_filling(sparse_matrix),binary_matrix,sparse_matrix)\n",
    "        fulling_matrix = fulling_matrix.reshape(-1,1,6,9).to(torch.float32)\n",
    "        full_m = sparse_matrix.reshape(-1,1,6,9).to(torch.float32)\n",
    "        fake_images,mu, logvar = G_net(fulling_matrix) # 生成的假的数据\n",
    "        fake_images2,mu2, logvar2 = G_net(full_m)\n",
    "\n",
    "#         print(real_data[0])\n",
    "#         print(fake_images[0])\n",
    "        MSEs = MSE_train(binary_matrix, real_data.reshape(-1,6,9) , fake_images.reshape(-1,6,9))\n",
    "        print(MSEs)\n",
    "        MSEs2 = MSE_train(binary_matrix, real_data.reshape(-1,6,9), fulling_matrix.reshape(-1,6,9))\n",
    "        print(MSEs2)\n",
    "        MSEs3 = MSE_train(binary_matrix, real_data.reshape(-1,6,9), miceforest_test(real_data.reshape(-1,6,9) * binary_matrix))\n",
    "        print(MSEs3)\n",
    "        MSEs4 = MSE_train(binary_matrix, real_data.reshape(-1,6,9), Missforest_test(real_data.reshape(-1,6,9) * binary_matrix))\n",
    "        print(MSEs4)\n",
    "        MSEs5 = MSE_train(binary_matrix, real_data.reshape(-1,6,9), imputee(real_data.reshape(-1,6,9) * binary_matrix))\n",
    "        print(MSEs5)\n",
    "        MSEs6 = MSE_train(binary_matrix, real_data.reshape(-1,6,9), fake_images2.reshape(-1,6,9))\n",
    "        print(MSEs6)\n",
    "        MSEs7 = MSE_train(binary_matrix, real_data.reshape(-1,6,9), imputee_linear(real_data.reshape(-1,6,9) * binary_matrix))\n",
    "        print(MSEs7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9dfcaf9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-bd69fe4fb5b3>:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(mat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0161, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0899, dtype=torch.float64)\n",
      "tensor(0.1146, dtype=torch.float64)\n",
      "tensor(0.0180, dtype=torch.float64)\n",
      "tensor(0.0165, dtype=torch.float64)\n",
      "tensor(0.0160, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0746, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "test(data_iter_test,Label2,G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d76469",
   "metadata": {},
   "source": [
    "用来尝试决策任务的好坏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "10a3776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def three2two(fake_images,label):\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import scale\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    fake_images = fake_images#.to(torch.float32)\n",
    "    fake_images = fake_images.detach().numpy().reshape(-1,9)\n",
    "    #j=scale(X=fake_images,with_mean=True,with_std=True,copy=True)\n",
    "    std=MinMaxScaler()\n",
    "    j=std.fit_transform(fake_images)\n",
    "    j = pd.DataFrame(j,columns=[str(k) for k in range(fake_images.shape[1])])\n",
    "#     j['54']=None\n",
    "#     for i in range(len(j)):\n",
    "#         if label[i//6].item()>0:\n",
    "#             j.loc[i,'54'] = 1\n",
    "#         else:\n",
    "#             j.loc[i,'54'] = 0\n",
    "    j['9']=None\n",
    "    for i in range(len(j)):\n",
    "        if label[i//6].item()>0:\n",
    "            j.loc[i,'9'] = 1\n",
    "        else:\n",
    "            j.loc[i,'9'] = 0\n",
    "    return j    \n",
    "\n",
    "def destree(df):\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.model_selection import KFold\n",
    "\n",
    "    \n",
    "    kf = KFold(5,shuffle = True)\n",
    "    \n",
    "    y = df.iloc[:,-1]\n",
    "    X = df.iloc[:,:-1]\n",
    "    #X = df.iloc[:,:9]\n",
    "    \n",
    "    results=[]\n",
    "    for train_index, test_index in kf.split(df):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        clf = DecisionTreeClassifier()\n",
    "        clf.fit(X_train, y_train.astype('int'))\n",
    "        y_test =  y_test.astype('int')\n",
    "        y_pred =  clf.predict(X_test)\n",
    "        y_pred = y_pred.astype('int')\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test,y_pred, pos_label=1)\n",
    "        auc=metrics.auc(fpr, tpr)\n",
    "        precision, recall, thresholds = precision_recall_curve(y_test,y_pred, pos_label=1)\n",
    "        auprc = metrics.auc(recall, precision)\n",
    "        y_pred=y_pred>0.5\n",
    "        acc=accuracy_score(y_test,y_pred)\n",
    "        f1 = f1_score(y_test,y_pred)\n",
    "        recall = recall_score(y_test,y_pred)\n",
    "        result=[acc,f1,recall,auc,auprc]\n",
    "        results.append(result)\n",
    "    results = pd.DataFrame(results,columns = ['acc','f1','recall','auc','auprc'])\n",
    "    return results  \n",
    "    #return clf.score(X_test, y_test.astype('int'))\n",
    "    \n",
    "def Decision_tree_task(data_iter, Label, G_net):\n",
    "    torch_dataset = Data.TensorDataset(data_iter, Label)\n",
    "    loader = Data.DataLoader(dataset = torch_dataset,batch_size=data.shape[0],shuffle=True)\n",
    "    \n",
    "    for x, (real_data,label) in enumerate(loader):\n",
    "        binary_matrix, sparse_matrix = matrix_init(real_data.reshape(-1,6,9))\n",
    "        fulling_matrix = SVD_func(means_filling(sparse_matrix),binary_matrix,sparse_matrix)\n",
    "        fulling_matrix = fulling_matrix.reshape(-1,1,6,9).to(torch.float32)\n",
    "        full_m = sparse_matrix.reshape(-1,1,6,9).to(torch.float32)\n",
    "        fake_images,mu, logvar = G_net(fulling_matrix) # 生成的假的数据\n",
    "        fake_images2,mu2, logvar2 = G_net(full_m)\n",
    "        \n",
    "        print(destree(three2two(real_data.reshape(-1,6,9),label)))\n",
    "        print(destree(three2two(fake_images,label)))\n",
    "        print(destree(three2two(fulling_matrix.reshape(-1,6,9),label)))\n",
    "        print(destree(three2two(miceforest_test(real_data.reshape(-1,6,9) * binary_matrix),label)))\n",
    "        print(destree(three2two(Missforest_test(real_data.reshape(-1,6,9) * binary_matrix),label)))\n",
    "        print(destree(three2two(imputee(real_data.reshape(-1,6,9) * binary_matrix),label)))\n",
    "        print(destree(three2two(fake_images2.reshape(-1,6,9),label)))\n",
    "        print(destree(three2two(imputee_linear(real_data.reshape(-1,6,9) * binary_matrix),label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "dc663d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def three2two(fake_images,label):\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import scale\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    fake_images = fake_images#.to(torch.float32)\n",
    "    fake_images = fake_images.detach().numpy().reshape(-1,9)\n",
    "    #j=scale(X=fake_images,with_mean=True,with_std=True,copy=True)\n",
    "    std=MinMaxScaler()\n",
    "    j=std.fit_transform(fake_images)\n",
    "    j = pd.DataFrame(j,columns=[str(k) for k in range(fake_images.shape[1])])\n",
    "#     j['54']=None\n",
    "#     for i in range(len(j)):\n",
    "#         if label[i//6].item()>0:\n",
    "#             j.loc[i,'54'] = 1\n",
    "#         else:\n",
    "#             j.loc[i,'54'] = 0\n",
    "    j['9']=None\n",
    "    for i in range(len(j)):\n",
    "        if label[i//6].item()>0:\n",
    "            j.loc[i,'9'] = 1\n",
    "        else:\n",
    "            j.loc[i,'9'] = 0\n",
    "    return j    \n",
    "\n",
    "def XGBdestree(df):\n",
    "    from sklearn import datasets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from xgboost import XGBClassifier\n",
    "    from xgboost import plot_importance\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.model_selection import KFold\n",
    "\n",
    "    \n",
    "    kf = KFold(5,shuffle = True)\n",
    "    \n",
    "    y = df.iloc[:,-1]\n",
    "    X = df.iloc[:,:-1]\n",
    "    #X = df.iloc[:,:9]\n",
    "    \n",
    "    results=[]\n",
    "    for train_index, test_index in kf.split(df):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        \n",
    "        model = XGBClassifier(learning_rate=0.1,\n",
    "                      #n_estimatores\n",
    "                      #含义：总共迭代的次数，即决策树的个数\n",
    "                      n_estimators=1000,\n",
    "                      #max_depth\n",
    "                      #含义：树的深度，默认值为6，典型值3-10。\n",
    "                      max_depth=6,\n",
    "                      #min_child_weight\n",
    "                      #调参：值越大，越容易欠拟合；值越小，越容易过拟合\n",
    "                      #（值较大时，避免模型学习到局部的特殊样本）。\n",
    "                      min_child_weight = 1,\n",
    "                      #惩罚项系数，指定节点分裂所需的最小损失函数下降值。\n",
    "                      gamma = 0,\n",
    "                      #subsample\n",
    "                      #含义：训练每棵树时，使用的数据占全部训练集的比例。\n",
    "                      # 默认值为1，典型值为0.5-1。\n",
    "                      subsample = 0.8,\n",
    "                      #colsample_bytree\n",
    "                      #含义：训练每棵树时，使用的特征占全部特征的比例。默认值为1，典型值为0.5-1。\n",
    "                      colsample_btree = 0.8,\n",
    "                      #objective 目标函数\n",
    "                      #multi：softmax num_class=n 返回类别\n",
    "                      objective = 'multi:softmax',\n",
    "                      #scale_pos_weight\n",
    "                      #正样本的权重，在二分类任务中，当正负样本比例失衡时，设置正样本的权重，模型效果更好。例如，当正负样本比例为1:10时，scale_pos_weight=10\n",
    "                      scale_pos_weight = 1,\n",
    "                      random_state= 27\n",
    "                      )\n",
    " \n",
    "        model.fit(x_train,\n",
    "          y_train,\n",
    "          eval_set=[(x_test,y_test)],\n",
    "          eval_metric = 'mlogloss',\n",
    "          early_stopping_rounds = 10,\n",
    "          verbose = True)\n",
    "        \n",
    "        y_test =  y_test.astype('int')\n",
    "        y_pred =  clf.predict(X_test)\n",
    "        y_pred = y_pred.astype('int')\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test,y_pred, pos_label=1)\n",
    "        auc=metrics.auc(fpr, tpr)\n",
    "        precision, recall, thresholds = precision_recall_curve(y_test,y_pred, pos_label=1)\n",
    "        auprc = metrics.auc(recall, precision)\n",
    "        y_pred=y_pred>0.5\n",
    "        acc=accuracy_score(y_test,y_pred)\n",
    "        f1 = f1_score(y_test,y_pred)\n",
    "        recall = recall_score(y_test,y_pred)\n",
    "        result=[acc,f1,recall,auc,auprc]\n",
    "        results.append(result)\n",
    "    results = pd.DataFrame(results,columns = ['acc','f1','recall','auc','auprc'])\n",
    "    return results  \n",
    "    #return clf.score(X_test, y_test.astype('int'))\n",
    "    \n",
    "def XGBtask(data_iter, Label, G_net):\n",
    "    torch_dataset = Data.TensorDataset(data_iter, Label)\n",
    "    loader = Data.DataLoader(dataset = torch_dataset,batch_size=data.shape[0],shuffle=True)\n",
    "    \n",
    "    for x, (real_data,label) in enumerate(loader):\n",
    "        binary_matrix, sparse_matrix = matrix_init(real_data.reshape(-1,6,9))\n",
    "        fulling_matrix = SVD_func(means_filling(sparse_matrix),binary_matrix,sparse_matrix)\n",
    "        fulling_matrix = fulling_matrix.reshape(-1,1,6,9).to(torch.float32)\n",
    "        full_m = sparse_matrix.reshape(-1,1,6,9).to(torch.float32)\n",
    "        fake_images,mu, logvar = G_net(fulling_matrix) # 生成的假的数据\n",
    "        fake_images2,mu2, logvar2 = G_net(full_m)\n",
    "        \n",
    "        print(destree(three2two(real_data.reshape(-1,6,9),label)))\n",
    "        print(destree(three2two(fake_images,label)))\n",
    "        print(destree(three2two(fulling_matrix.reshape(-1,6,9),label)))\n",
    "        print(destree(three2two(miceforest_test(real_data.reshape(-1,6,9) * binary_matrix),label)))\n",
    "        print(destree(three2two(Missforest_test(real_data.reshape(-1,6,9) * binary_matrix),label)))\n",
    "        print(destree(three2two(imputee(real_data.reshape(-1,6,9) * binary_matrix),label)))\n",
    "        print(destree(three2two(fake_images2.reshape(-1,6,9),label)))\n",
    "        print(destree(three2two(imputee_linear(real_data.reshape(-1,6,9) * binary_matrix),label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0f5e7cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-bd69fe4fb5b3>:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(mat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        acc        f1    recall       auc     auprc\n",
      "0  0.802778  0.336449  0.290323  0.599859  0.406272\n",
      "1  0.797222  0.342342  0.372549  0.619931  0.389052\n",
      "2  0.847222  0.466019  0.470588  0.689987  0.503563\n",
      "3  0.800000  0.250000  0.235294  0.564249  0.305147\n",
      "4  0.769444  0.394161  0.442623  0.639372  0.446165\n",
      "        acc        f1    recall       auc     auprc\n",
      "0  0.844444  0.508772  0.517857  0.711231  0.546429\n",
      "1  0.847222  0.504505  0.538462  0.718906  0.539852\n",
      "2  0.827778  0.465517  0.450000  0.676667  0.511905\n",
      "3  0.825000  0.479339  0.467742  0.683535  0.525467\n",
      "4  0.869444  0.515464  0.543478  0.730338  0.546004\n",
      "        acc        f1    recall       auc     auprc\n",
      "0  0.850000  0.578125  0.587302  0.746513  0.614377\n",
      "1  0.836111  0.458716  0.446429  0.677162  0.502119\n",
      "2  0.861111  0.456522  0.411765  0.673520  0.503647\n",
      "3  0.863889  0.588235  0.636364  0.770641  0.619397\n",
      "4  0.861111  0.553571  0.607843  0.755378  0.585798\n",
      "        acc        f1    recall       auc     auprc\n",
      "0  0.738889  0.241935  0.283019  0.550304  0.299921\n",
      "1  0.747222  0.222222  0.224138  0.535910  0.284738\n",
      "2  0.763889  0.174757  0.200000  0.522222  0.227586\n",
      "3  0.761111  0.245614  0.245614  0.551850  0.305336\n",
      "4  0.736111  0.188034  0.174603  0.514911  0.261376\n",
      "        acc        f1    recall       auc     auprc\n",
      "0  0.758333  0.186916  0.181818  0.522057  0.249563\n",
      "1  0.736111  0.188034  0.177419  0.514884  0.259543\n",
      "2  0.786111  0.306306  0.309091  0.590611  0.359109\n",
      "3  0.744444  0.148148  0.173913  0.500969  0.204250\n",
      "4  0.733333  0.142857  0.137931  0.492807  0.212484\n",
      "        acc        f1    recall       auc     auprc\n",
      "0  0.788889  0.191489  0.187500  0.534455  0.245743\n",
      "1  0.741667  0.146789  0.150943  0.497296  0.209400\n",
      "2  0.716667  0.215385  0.229508  0.522781  0.281481\n",
      "3  0.750000  0.224138  0.224138  0.537566  0.286638\n",
      "4  0.766667  0.288136  0.303571  0.577773  0.343049\n",
      "        acc        f1    recall       auc     auprc\n",
      "0  0.877778  0.568627  0.508772  0.727983  0.615497\n",
      "1  0.844444  0.416667  0.416667  0.663462  0.455556\n",
      "2  0.808333  0.420168  0.416667  0.651667  0.468809\n",
      "3  0.850000  0.500000  0.473684  0.697238  0.543215\n",
      "4  0.844444  0.471698  0.462963  0.687364  0.512144\n",
      "        acc        f1    recall       auc     auprc\n",
      "0  0.802778  0.297030  0.294118  0.590425  0.347059\n",
      "1  0.797222  0.317757  0.326923  0.601773  0.366618\n",
      "2  0.808333  0.439024  0.421875  0.656883  0.491140\n",
      "3  0.822222  0.346939  0.314815  0.613290  0.401978\n",
      "4  0.791667  0.324324  0.327273  0.601341  0.375740\n"
     ]
    }
   ],
   "source": [
    "XGBtask(data_iter_test,Label2,G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8157b108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-bd69fe4fb5b3>:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(mat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        acc        f1    recall       auc     auprc\n",
      "0  0.794444  0.350877  0.327869  0.608750  0.409558\n",
      "1  0.822222  0.384615  0.476190  0.672058  0.429941\n",
      "2  0.777778  0.354839  0.343750  0.607686  0.413542\n",
      "3  0.761111  0.218182  0.255319  0.546190  0.271509\n",
      "4  0.811111  0.433333  0.419355  0.655986  0.483815\n",
      "        acc        f1    recall       auc     auprc\n",
      "0  0.863889  0.566372  0.551724  0.737783  0.602882\n",
      "1  0.847222  0.455446  0.450980  0.681801  0.494379\n",
      "2  0.861111  0.576271  0.596491  0.753691  0.608879\n",
      "3  0.847222  0.476190  0.446429  0.683741  0.521372\n",
      "4  0.855556  0.509434  0.500000  0.709150  0.547115\n",
      "        acc        f1    recall       auc     auprc\n",
      "0  0.786111  0.393701  0.403226  0.634499  0.445309\n",
      "1  0.783333  0.338983  0.400000  0.622581  0.388725\n",
      "2  0.808333  0.389381  0.415094  0.645658  0.433936\n",
      "3  0.827778  0.483333  0.483333  0.690000  0.526389\n",
      "4  0.783333  0.264151  0.274510  0.570912  0.315917\n",
      "        acc        f1    recall       auc     auprc\n",
      "0  0.822222  0.438596  0.431034  0.664193  0.484565\n",
      "1  0.763889  0.273504  0.258065  0.563596  0.338376\n",
      "2  0.775000  0.242991  0.265306  0.560306  0.294722\n",
      "3  0.825000  0.452174  0.530612  0.700997  0.494220\n",
      "4  0.800000  0.389831  0.396552  0.637018  0.438554\n",
      "        acc        f1    recall       auc     auprc\n",
      "0  0.797222  0.342342  0.327586  0.607502  0.397205\n",
      "1  0.791667  0.369748  0.407407  0.633442  0.417379\n",
      "2  0.816667  0.440678  0.433333  0.663333  0.488027\n",
      "3  0.800000  0.320755  0.340000  0.607097  0.367619\n",
      "4  0.794444  0.327273  0.333333  0.604575  0.377381\n",
      "        acc        f1    recall       auc     auprc\n",
      "0  0.777778  0.344262  0.355932  0.608199  0.397411\n",
      "1  0.811111  0.358491  0.333333  0.617162  0.413322\n",
      "2  0.808333  0.410256  0.436364  0.655887  0.454786\n",
      "3  0.769444  0.325203  0.344828  0.597910  0.379038\n",
      "4  0.805556  0.300000  0.319149  0.598872  0.345528\n",
      "        acc        f1    recall       auc     auprc\n",
      "0  0.847222  0.485981  0.456140  0.688466  0.531126\n",
      "1  0.794444  0.412698  0.472727  0.662593  0.459740\n",
      "2  0.844444  0.461538  0.444444  0.679739  0.503889\n",
      "3  0.830556  0.460177  0.481481  0.686819  0.499969\n",
      "4  0.822222  0.428571  0.428571  0.661654  0.473016\n",
      "        acc        f1    recall       auc     auprc\n",
      "0  0.802778  0.422764  0.412698  0.649110  0.474405\n",
      "1  0.783333  0.327586  0.358491  0.607584  0.377261\n",
      "2  0.822222  0.500000  0.533333  0.706667  0.540850\n",
      "3  0.805556  0.406780  0.452830  0.659640  0.451308\n",
      "4  0.802778  0.323810  0.361702  0.615356  0.369069\n"
     ]
    }
   ],
   "source": [
    "Decision_tree_task(data_iter_test,Label2,G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4b4b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def init_weights(m):\n",
    "#     if type(m) == nn.Linear:\n",
    "#         nn.init.normal_(m.weight, std=0.01)\n",
    "\n",
    "# def mlp_net(matrix,label):\n",
    "#     #matrix = matrix.to(torch.float64)\n",
    "#     #label = label.to(torch.float64)\n",
    "#     net = nn.Sequential(nn.Flatten(),\n",
    "#                     nn.Linear(54, 128),\n",
    "#                     nn.ReLU(),\n",
    "#                     nn.Linear(128, 32),\n",
    "#                     nn.ReLU(),\n",
    "#                     nn.Linear(32,8),\n",
    "#                     nn.ReLU(),\n",
    "#                     nn.Linear(8,1)\n",
    "#                     )\n",
    "#     net.apply(init_weights)\n",
    "#     batch_size, lr, num_epochs = 300, 0.1, 150\n",
    "# #     loss = nn.CrossEntropyLoss()\n",
    "#     loss = nn.BCEWithLogitsLoss()\n",
    "#     trainer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    \n",
    "#     matrix = matrix.reshape(-1,6,9)\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         l = loss(net(matrix[:204]) ,label[:204].reshape(204,-1))\n",
    "#         trainer.zero_grad()\n",
    "#         l.backward(retain_graph=True)\n",
    "#         trainer.step()\n",
    "#         l = loss(net(matrix[:204]) ,label[:204].reshape(204,-1))\n",
    "# #         if epoch%10==0:\n",
    "# #             print(f'epoch {epoch + 1}, loss {l:f}')\n",
    "#     l = loss(net(matrix[204:]) ,label[204:].reshape(96,-1))\n",
    "#     print(l)\n",
    "    \n",
    "# def mlp_task(data_iter, Label, G_net):\n",
    "#     torch_dataset = Data.TensorDataset(data_iter, Label)\n",
    "#     loader = Data.DataLoader(dataset = torch_dataset,batch_size=data.shape[0],shuffle=True)\n",
    "    \n",
    "#     for x, (real_data,label) in enumerate(loader):\n",
    "#         binary_matrix, sparse_matrix = matrix_init(real_data.reshape(-1,6,9))\n",
    "#         fulling_matrix = SVD_func(means_filling(sparse_matrix),binary_matrix,sparse_matrix)\n",
    "#         fulling_matrix = fulling_matrix.reshape(-1,1,6,9).to(torch.float32)\n",
    "#         fake_images,mu, logvar = G_net(fulling_matrix) # 生成的假的数据\n",
    "        \n",
    "#         mlp_net(fake_images,label)\n",
    "#         mlp_net(fulling_matrix.reshape(-1,6,9),label)\n",
    "#         mlp_net(miceforest_test(real_data.reshape(-1,6,9) * binary_matrix),label)\n",
    "#         mlp_net(Missforest_test(real_data.reshape(-1,6,9) * binary_matrix),label)\n",
    "#         mlp_net(imputee(real_data.reshape(-1,6,9) * binary_matrix),label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2fcb0a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlp_task(data_iter_test,Label2,G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed0ae19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def three2two54(fake_images,label):\n",
    "    fake_images = fake_images.to(torch.float32)\n",
    "    fake_images = fake_images.reshape(-1,54)\n",
    "    j = fake_images.detach().numpy()\n",
    "    j = pd.DataFrame(j,columns=[str(k) for k in range(fake_images.shape[1])])\n",
    "    j['54']=None\n",
    "    for i in range(len(j)):\n",
    "        if label[i].item()>0:\n",
    "            j.loc[i,'54'] = 1\n",
    "        else:\n",
    "            j.loc[i,'54'] = 0\n",
    "    return j    \n",
    "\n",
    "def mlp_(df):\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.model_selection import KFold\n",
    "    \n",
    "    kf = KFold(5,shuffle = True)\n",
    "    y = df.iloc[:,-1]\n",
    "    X = df.iloc[:,:-1]\n",
    "    print(y.sum())\n",
    "    \n",
    "    results=[]\n",
    "    for train_index, test_index in kf.split(df):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        clf = MLPClassifier(activation='relu',max_iter=2000,\n",
    "       hidden_layer_sizes=(32,16,8,4),random_state=3, shuffle=True,\n",
    "       solver='adam')\n",
    "        clf.fit(X_train, y_train.astype('int'))\n",
    "        y_test =  y_test.astype('int')\n",
    "        y_pred =  clf.predict(X_test)\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test,y_pred, pos_label=1)\n",
    "        auc=metrics.auc(fpr, tpr)\n",
    "        precision, recall, thresholds = precision_recall_curve(y_test,y_pred, pos_label=1)\n",
    "        auprc = metrics.auc(recall, precision)\n",
    "        y_pred=y_pred>0.5\n",
    "        acc=accuracy_score(y_test,y_pred)\n",
    "        f1 = f1_score(y_test,y_pred)\n",
    "        recall = recall_score(y_test,y_pred)\n",
    "        result=[acc,f1,recall,auc,auprc]\n",
    "        results.append(result)\n",
    "    results = pd.DataFrame(results,columns = ['acc','f1','recall','auc','auprc'])\n",
    "    print(clf.score(X_test, y_test.astype('int')))\n",
    "    return results  \n",
    "    #return clf.score(X_test, y_test.astype('int'))\n",
    "    \n",
    "def mlpp_task(data_iter, Label, G_net):\n",
    "    torch_dataset = Data.TensorDataset(data_iter, Label)\n",
    "    loader = Data.DataLoader(dataset = torch_dataset,batch_size=data.shape[0],shuffle=True)\n",
    "    \n",
    "    for x, (real_data,label) in enumerate(loader):\n",
    "        binary_matrix, sparse_matrix = matrix_init(real_data.reshape(-1,6,9))\n",
    "        fulling_matrix = SVD_func(means_filling(sparse_matrix),binary_matrix,sparse_matrix)\n",
    "        fulling_matrix = fulling_matrix.reshape(-1,1,6,9).to(torch.float32)\n",
    "        fake_images,mu, logvar = G_net(fulling_matrix) # 生成的假的数据\n",
    "        \n",
    "        print(fake_images.shape)\n",
    "        print(mlp_(three2two54(fake_images,label)))\n",
    "        print(mlp_(three2two54(fulling_matrix.reshape(-1,6,9),label)))\n",
    "        print(mlp_(three2two54(miceforest_test(real_data.reshape(-1,6,9) * binary_matrix),label)))\n",
    "        print(mlp_(three2two54(Missforest_test(real_data.reshape(-1,6,9) * binary_matrix),label)))\n",
    "        print(mlp_(three2two54(imputee(real_data.reshape(-1,6,9) * binary_matrix),label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe7da810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-bd69fe4fb5b3>:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(mat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 1, 6, 9])\n",
      "46\n",
      "0.8666666666666667\n",
      "        acc   f1  recall  auc     auprc\n",
      "0  0.883333  0.0     0.0  0.5  0.558333\n",
      "1  0.816667  0.0     0.0  0.5  0.591667\n",
      "2  0.883333  0.0     0.0  0.5  0.558333\n",
      "3  0.783333  0.0     0.0  0.5  0.608333\n",
      "4  0.866667  0.0     0.0  0.5  0.566667\n",
      "46\n",
      "0.7\n",
      "        acc        f1    recall       auc     auprc\n",
      "0  0.866667  0.333333  0.250000  0.605769  0.425000\n",
      "1  0.833333  0.166667  0.111111  0.535948  0.288889\n",
      "2  0.816667  0.266667  0.200000  0.570000  0.366667\n",
      "3  0.816667  0.266667  0.250000  0.576923  0.317857\n",
      "4  0.700000  0.100000  0.090909  0.463822  0.184343\n",
      "46\n",
      "0.8\n",
      "        acc   f1  recall       auc     auprc\n",
      "0  0.916667  0.0     0.0  0.500000  0.541667\n",
      "1  0.883333  0.0     0.0  0.490741  0.050000\n",
      "2  0.816667  0.0     0.0  0.500000  0.591667\n",
      "3  0.800000  0.0     0.0  0.500000  0.600000\n",
      "4  0.800000  0.0     0.0  0.500000  0.600000\n",
      "46\n",
      "0.8166666666666667\n",
      "        acc        f1    recall       auc     auprc\n",
      "0  0.800000  0.250000  0.166667  0.562500  0.416667\n",
      "1  0.800000  0.333333  0.300000  0.600000  0.395833\n",
      "2  0.816667  0.000000  0.000000  0.453704  0.050000\n",
      "3  0.816667  0.000000  0.000000  0.490000  0.083333\n",
      "4  0.816667  0.153846  0.125000  0.524038  0.220833\n",
      "46\n",
      "0.8\n",
      "        acc        f1    recall       auc     auprc\n",
      "0  0.833333  0.166667  0.100000  0.540000  0.375000\n",
      "1  0.800000  0.000000  0.000000  0.500000  0.600000\n",
      "2  0.833333  0.166667  0.142857  0.533693  0.221429\n",
      "3  0.833333  0.166667  0.125000  0.533654  0.245833\n",
      "4  0.800000  0.333333  0.333333  0.607843  0.383333\n"
     ]
    }
   ],
   "source": [
    "mlpp_task(data_iter_test,Label2,G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "42e37381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def three2two54(fake_images,label):\n",
    "    fake_images = fake_images.to(torch.float32)\n",
    "    fake_images = fake_images.reshape(-1,54)\n",
    "    j = fake_images.detach().numpy()\n",
    "    print(j)\n",
    "    j = pd.DataFrame(j,columns=[str(k) for k in range(fake_images.shape[1])])\n",
    "    j['54']=None\n",
    "    for i in range(len(j)):\n",
    "        if label[i].item()>0:\n",
    "            j.loc[i,'54'] = 1\n",
    "        else:\n",
    "            j.loc[i,'54'] = 0\n",
    "    return j    \n",
    "\n",
    "def svm_(df):\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.preprocessing import label_binarize\n",
    "    from sklearn.model_selection import KFold\n",
    "    \n",
    "    kf = KFold(5,shuffle = True)\n",
    "    \n",
    "    y = df.iloc[:,-1]\n",
    "    X = df.iloc[:,:-1]\n",
    "    \n",
    "    print(df)\n",
    "    results=[]\n",
    "    for train_index, test_index in kf.split(df):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        print(X_train.shape,X_test.shape)\n",
    "        clf = LinearSVC(penalty='l2', loss='squared_hinge',random_state=10)\n",
    "        clf.fit(X_train, y_train.astype('int'))\n",
    "        y_test =  y_test.astype('int')\n",
    "        y_pred =  clf.predict(X_test)\n",
    "        y_test = np.array(y_test).reshape(y_pred.shape)\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test,y_pred, pos_label=1)\n",
    "        auc=metrics.auc(fpr, tpr)\n",
    "        precision, recall, thresholds = precision_recall_curve(y_test,y_pred, pos_label=1)\n",
    "        auprc = metrics.auc(recall, precision)\n",
    "        #y_pred=y_pred>0.5\n",
    "        acc=accuracy_score(y_test,y_pred)\n",
    "        f1 = f1_score(y_test,y_pred)\n",
    "        recall = recall_score(y_test,y_pred)\n",
    "        result=[acc,f1,recall,auc,auprc]\n",
    "        results.append(result)\n",
    "    results = pd.DataFrame(results,columns = ['acc','f1','recall','auc','auprc'])\n",
    "    print(clf.score(X_test, y_test.astype('int')))\n",
    "    return results  \n",
    "    #return clf.score(X_test, y_test.astype('int'))\n",
    "    \n",
    "def svm_task(data_iter, Label, G_net):\n",
    "    torch_dataset = Data.TensorDataset(data_iter_train, Label)\n",
    "    loader = Data.DataLoader(dataset = torch_dataset,batch_size=data.shape[0],shuffle=True)\n",
    "    \n",
    "    for x, (real_data,label) in enumerate(loader):\n",
    "        binary_matrix, sparse_matrix = matrix_init(real_data.reshape(-1,6,9))\n",
    "        fulling_matrix = SVD_func(means_filling(sparse_matrix),binary_matrix,sparse_matrix)\n",
    "        fulling_matrix = fulling_matrix.reshape(-1,1,6,9).to(torch.float32)\n",
    "        fake_images,mu, logvar = G_net(fulling_matrix) # 生成的假的数据\n",
    "        \n",
    "        print(svm_(three2two54(fake_images,label)))\n",
    "        print(svm_(three2two54(fulling_matrix.reshape(-1,6,9),label)))\n",
    "        print(svm_(three2two54(miceforest_test(real_data.reshape(-1,6,9) * binary_matrix),label)))\n",
    "        print(svm_(three2two54(Missforest_test(real_data.reshape(-1,6,9) * binary_matrix),label)))\n",
    "        print(svm_(three2two54(imputee(real_data.reshape(-1,6,9) * binary_matrix),label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0a6f3a1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Size mismatch between tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-66b274d2ed70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msvm_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iter_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-45-9c0550da8998>\u001b[0m in \u001b[0;36msvm_task\u001b[0;34m(data_iter, Label, G_net)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msvm_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mtorch_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iter_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Size mismatch between tensors\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Size mismatch between tensors"
     ]
    }
   ],
   "source": [
    "svm_task(data_iter_test,Label,G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04c7d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7fc87c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
